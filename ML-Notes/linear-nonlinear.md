### 1. What "linear" means

* **Linear regression** = model is **linear in its parameters (Œ≤‚Äôs)**.
* Features (x‚Äôs) can be transformed (squared, cubed, log, sqrt, etc.), but as long as Œ≤‚Äôs appear linearly, it‚Äôs still linear regression.

General form:

$$
y = Œ≤‚ÇÄ + Œ≤‚ÇÅf_1(x) + Œ≤‚ÇÇf_2(x) + \dots + Œ≤‚Çôf_n(x)
$$

---

### 2. Examples

* **Linear regression (straight line):**

  $$
  y = Œ≤‚ÇÄ + Œ≤‚ÇÅx
  $$

* **Polynomial regression (parabola, cubic, etc.):**

  $$
  y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx^2 + Œ≤‚ÇÉx^3
  $$

  Still linear regression (linear in Œ≤‚Äôs).

* **Log-transformed regression:**

  $$
  y = Œ≤‚ÇÄ + Œ≤‚ÇÅ\log(x)
  $$

  Still linear in Œ≤‚Äôs.

* **Nonlinear regression (true nonlinear):**

  $$
  y = Œ≤‚ÇÄ + e^{Œ≤‚ÇÅx} \quad \text{or} \quad y = Œ≤‚ÇÄ + (Œ≤‚ÇÅ)^2x
  $$

  Nonlinear in Œ≤‚Äôs ‚Üí needs different optimization.

---

### 3. Polynomial Regression

* Technique: expand features into polynomial terms ($x, x^2, x^3, ...$) and run linear regression.
* Used to fit curves (parabola, cubic, etc.).
* Still solved with standard linear regression (ordinary least squares).

---
### 1. Underfitting in Polynomial Regression

-   **Underfitting** = model is too simple to capture the true relationship.
    
-   Example: if the true relation is
    
    y=2x3+x2+5y = 2x^3 + x^2 + 5y=2x3+x2+5
    
    but you fit only a **linear model**
    
    y=Œ≤0+Œ≤1xy = Œ≤‚ÇÄ + Œ≤‚ÇÅxy=Œ≤0‚Äã+Œ≤1‚Äãx
    
    or a **quadratic model**
    
    y=Œ≤0+Œ≤1x+Œ≤2x2y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx^2y=Œ≤0‚Äã+Œ≤1‚Äãx+Œ≤2‚Äãx2
    
    ‚Üí the model will **miss the cubic pattern**, giving biased predictions.
    

----------

### 2. How to control underfitting

-   **Increase model complexity**:
    
    -   Use higher polynomial degree (x3,x4...x^3, x^4...x3,x4...) if needed.
        
    -   Example: going from degree=2 (parabola) to degree=3 (cubic) reduces underfitting if the true relation is cubic.
        
-   **Add more features**:
    
    -   If the relation depends on more variables, not just xxx, include them.
        
-   **Check data transformations**:
    
    -   Sometimes log, sqrt, or interaction terms (x1√óx2x_1 \times x_2x1‚Äã√óx2‚Äã) better capture the relationship.
        

----------

### 3. Balance of underfitting vs overfitting

-   **Too low degree** ‚Üí underfitting.
    
-   **Too high degree** ‚Üí overfitting.
    
-   **Right degree + regularization** ‚Üí good generalization.
    

This balance is often found using **cross-validation** (testing models on held-out data).

----------

üëâ So:

-   **Underfitting** in polynomial regression is controlled by **increasing the degree of polynomial features** (making the model more flexible).
    
-   **Overfitting** is controlled by **regularization** (Ridge/Lasso) or reducing degree.

### 4. Why polynomial regression may not be optimal

* **Underfitting** if the real relation is more complex.
* **Overfitting** if the degree is too high (model fits noise).
* **Numerical issues** if features like $x^n$ get very large (fix with scaling/normalization).

Regularization (Ridge, Lasso) can help stabilize polynomial models.

---

### 5. Other regression types

* **Linear Regression** ‚Äì straight line.
* **Polynomial Regression** ‚Äì curved, via polynomial features.
* **Logistic Regression** ‚Äì classification, not regression.
* **Ridge/Lasso/ElasticNet** ‚Äì linear regression with regularization.
* **True Nonlinear Regression** ‚Äì parameters inside nonlinear functions (needs iterative solvers).
* **Tree-based & Neural Networks** ‚Äì flexible nonlinear models without explicit polynomial features.

---

### 6. Optimization and minima

* **Linear regression loss (MSE)** is convex ‚Üí only one minimum (global minimum). Always solvable exactly.
* **Nonlinear regression loss** can have multiple valleys ‚Üí optimizer may stop at a **local minimum** instead of the global one.

Analogy:

* Global minimum = lowest valley in the entire mountain range.
* Local minimum = a smaller dip nearby that isn‚Äôt the deepest.

---

üëâ Key Takeaway:

* Linear regression = linear in parameters, not features.
* Polynomial regression = still linear regression, but features are powers of x.
* Nonlinear regression (true) = when parameters themselves appear nonlinearly.
* Polynomial regression works well for simple curves, but for complex patterns you may need regularization, trees, or neural networks.
